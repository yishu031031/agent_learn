# 智能体经典范式构建

- 一个现代的智能体，其核心能力在于能将大语言模型的推理能力与外部世界联通。
- 它能够自主地理解用户意图、拆解复杂任务，并通过调用代码解释器、搜索引擎、API等一系列“工具”，来获取信息、执行操作，最终达成目标。 
- 然而，智能体并非万能，它同样面临着来自大模型本身的“幻觉”问题、在复杂任务中可能陷入推理循环、以及对工具的错误使用等挑战，这些也构成了智能体的能力边界。

## 智能体经典范式构建
- 为了更好地组织智能体的“思考”与“行动”过程，介绍以下三种智能体架构：
- 1. ReAct
- 2. Plan-and-Solve
- 3. Reflection

---

### ReAct 范式简介

ReAct 是一种将**思考 (Reasoning)** 和**行动 (Acting)** 紧密结合的范式，使智能体能够“边想边做”，并根据环境反馈动态调整策略。

在 ReAct 提出之前，主流的 LLM 方法主要分为两类：
* **“纯思考”型**（如 Chain-of-Thought）：擅长逻辑推理，但无法与外部世界交互，易产生幻觉。
* **“纯行动”型**：模型直接输出动作，但缺乏规划、记忆和自我纠错能力。

> **核心理念**：ReAct 认为思考与行动是相辅相成的。思考指导行动，行动的结果（Observation）又反过来修正思考。

#### ReAct 的核心三元组（Trajectories）

ReAct 通过循环执行以下三个步骤来解决问题：

1.  **思考轨迹 (Thought)**
    这是智能体的“内心独白”。它分析当前情况、分解任务、制定下一步计划，或者反思上一步的结果。

2.  **行动轨迹 (Action)**
    这是智能体决定采取的具体动作，通常是调用外部工具或API。
    * 例如：`Search['华为最新款手机']`

3.  **观察/反馈轨迹 (Observation)**
    这是智能体执行行动后，从环境或工具获得的真实返回结果。它将被用于修正下一轮的思考，引导智能体朝着正确的方向收敛。

**智能体将不断重复这个 Thought -> Action -> Observation 的循环，将新的观察结果追加到历史记录中，形成一个不断增长的上下文，直到它在Thought中认为已经找到了最终答案，然后输出结果。这个过程形成了一个强大的协同效应：推理使得行动更具目的性，而行动则为推理提供了事实依据。**

我们可以将这个过程形式化地表达出来，如图 image中的RaAct.png 所示。具体来说，在每个时间步 $t$，智能体的策略（即大语言模型 $\pi$）会根据初始问题 $q$ 和之前所有步骤的“行动-观察”历史轨迹 $((a_1, o_1), \dots, (a_{t-1}, o_{t-1}))$，来生成当前的思考 $th_t$ 和行动 $a_t$：

$$
(th_t, a_t) = \pi(q, (a_1, o_1), \dots, (a_{t-1}, o_{t-1}))
$$

随后，环境中的工具 $T$ 会执行行动 $a_t$，并返回一个新的观察结果 $o_t$：

$$
o_t = T(a_t)
$$

这个循环不断进行，将新的 $(a_t, o_t)$ 对追加到历史中，直到模型在思考 $th_t$ 中判断任务已完成。

#### ReAct 适用场景
- ReAct 特别适用于需要动态交互和多步推理的复杂任务，如：
1.  需要外部知识的任务：如查询实时信息（天气、新闻、股价）、搜索专业领域的知识等。

2.  需要精确计算的任务：将数学问题交给计算器工具，避免LLM的计算错误。

3.  需要与API交互的任务：如操作数据库、调用某个服务的API来完成特定功能。

#### ReAct 特点与不足

##### 特点
1. 高可解释性：ReAct 最大的优点之一就是透明。通过 Thought 链，我们可以清晰地看到智能体每一步的“心路历程”——它为什么会选择这个工具，下一步又打算做什么。这对于理解、信任和调试智能体的行为至关重要。
2. 动态规划与纠错能力：与一次性生成完整计划的范式不同，ReAct 是“走一步，看一步”。它根据每一步从外部世界获得的 Observation 来动态调整后续的 Thought 和 Action。如果上一步的搜索结果不理想，它可以在下一步中修正搜索词，重新尝试。
3. 工具协同能力：ReAct 范式天然地将大语言模型的推理能力与外部工具的执行能力结合起来。LLM 负责运筹帷幄（规划和推理），工具负责解决具体问题（搜索、计算），二者协同工作，突破了单一 LLM 在知识时效性、计算准确性等方面的固有局限。

##### 不足
1. 对LLM自身能力的强依赖：ReAct 流程的成功与否，高度依赖于底层 LLM 的综合能力。如果 LLM 的逻辑推理能力、指令遵循能力或格式化输出能力不足，就很容易在 Thought 环节产生错误的规划，或者在 Action 环节生成不符合格式的指令，导致整个流程中断。
2. 执行效率问题：由于其循序渐进的特性，完成一个任务通常需要多次调用 LLM。每一次调用都伴随着网络延迟和计算成本。对于需要很多步骤的复杂任务，这种串行的“思考-行动”循环可能会导致较高的总耗时和费用。
3. 提示词的脆弱性：整个机制的稳定运行建立在一个精心设计的提示词模板之上。模板中的任何微小变动，甚至是用词的差异，都可能影响 LLM 的行为。此外，并非所有模型都能持续稳定地遵循预设的格式，这增加了在实际应用中的不确定性。
4. 可能陷入局部最优：步进式的决策模式意味着智能体缺乏一个全局的、长远的规划。它可能会因为眼前的 Observation 而选择一个看似正确但长远来看并非最优的路径，甚至在某些情况下陷入“原地打转”的循环中。

---
### Plan-and-Solve
- 先规划再执行的范式，智能体先思考一个解决方案，然后根据计划执行，是一种三思而后行的范式

---
### Reflection
- 一种基于反馈的智能体架构，智能体在执行任务时，会根据任务完成度和结果，进行反思和调整，以提高任务执行效率和质量

## Langchian与LlamIndex等智能体框架
- 为了更方便地实现以上三种智能体架构，涌现出了许多智能体框架，如Langchain、LlamIndex等。
- 这些框架提供了丰富的组件和工具，帮助开发者快速构建、部署和管理智能体系统。
- 同时，它们也关注智能体的可扩展性、可维护性和可解释性，以满足不同场景下的需求。

**本章节我们将重点关注三种基础智能体架构，对底层结构有一个更好的了解**

## 环境搭建
- 在你的终端中运行以下命令：
```bash
pip install numpy pandas
```

## 工具定义和管理
- 智能体通过调用外部工具来获取信息或执行操作，因此需要定义和管理这些工具。
- 实现工具一般分布进行，首先实现工具的核心功能，然后构建一个通用的工具管理器

---
### 工具的定义
- 一个良好定义的工具应包含以下三个核心要素：
1. 名称(Name):一个简洁、唯一的标识符，供智能体在 Action 中调用，例如 Search。
2. 描述 (Description)： 一段清晰的自然语言描述，说明这个工具的用途。这是整个机制中最关键的部分，因为大语言模型会依赖这段描述来判断何时使用哪个工具。
3. 执行逻辑 (Execution Logic)： 真正执行任务的函数或方法。



# æ–‡ä»¶å: agent.py
import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage

# å¯¼å…¥ MCP æ ¸å¿ƒåº“
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

load_dotenv(override=True)

async def run_agent():
    # ==========================================
    # 1. é…ç½®è¿æ¥ï¼šå‘Šè¯‰ Agent å»å“ªé‡Œæ‰¾ server.py
    # ==========================================
    server_params = StdioServerParameters(
        command="python", # ä½¿ç”¨ python å‘½ä»¤
        args=["my_server.py"], # æ‰§è¡Œ server.py è„šæœ¬
        env=None
    )

    # ==========================================
    # 2. å»ºç«‹è¿æ¥ (Context Manager)
    # ==========================================
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            
            # --- æ­¥éª¤ A: åˆå§‹åŒ–å¹¶è·å–å·¥å…·åˆ—è¡¨ ---
            await session.initialize()
            
            # ä»æœåŠ¡ç«¯è·å–å¯ç”¨å·¥å…·
            tools_list = await session.list_tools()
            print(f"ğŸ”Œ å·²è¿æ¥ MCP æœåŠ¡ï¼Œå‘ç°å·¥å…·: {[t.name for t in tools_list.tools]}")

            # --- æ­¥éª¤ B: å°† MCP å·¥å…·è½¬æ¢ä¸º LLM å¯ç†è§£çš„æ ¼å¼ ---
            # æˆ‘ä»¬éœ€è¦æŠŠ MCP çš„ schema è½¬æ¢æˆ OpenAI çš„ tool schema
            formatted_tools = []
            for tool in tools_list.tools:
                formatted_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.inputSchema
                    }
                })

            # --- æ­¥éª¤ C: åˆå§‹åŒ– LLM å¹¶ç»‘å®šå·¥å…· ---
            llm = ChatOpenAI(
                model = os.getenv("LLM_MODEL_ID","gpt-4o"),
                api_key = os.getenv("LLM_API_KEY"),
                base_url = os.getenv("LLM_BASE_URL","https://api.example-llm.com/v1"),
                temperature=0.7,
            )
            llm_with_tools = llm.bind_tools(formatted_tools)

            # --- æ­¥éª¤ D: å‘é€ç”¨æˆ·æŸ¥è¯¢ ---
            query = "è¯·å¸®æˆ‘è®¡ç®— 123 åŠ  456 æ˜¯å¤šå°‘ï¼Ÿç„¶åå†åˆ†æä¸€ä¸‹å­—ç¬¦ä¸² 'Hello MCP World' çš„é•¿åº¦ã€‚"
            print(f"\nğŸ¤” ç”¨æˆ·æé—®: {query}")
            
            messages = [HumanMessage(content=query)]
            ai_response = await llm_with_tools.ainvoke(messages)
            
            messages.append(ai_response) # æŠŠ AI çš„æ€è€ƒåŠ å…¥å†å²

            # --- æ­¥éª¤ E: å¤„ç†å·¥å…·è°ƒç”¨ (Tool Call) ---
            if ai_response.tool_calls:
                for tool_call in ai_response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]
                    
                    print(f"ğŸ› ï¸ Agent å†³å®šè°ƒç”¨å·¥å…·: {tool_name} å‚æ•°: {tool_args}")

                    # !!! å…³é”®ç‚¹ï¼šAgent é€šè¿‡ Session è°ƒç”¨è¿œç«¯ MCP Server !!!
                    result = await session.call_tool(tool_name, tool_args)
                    
                    # è·å–ç»“æœæ–‡æœ¬
                    tool_output = result.content[0].text
                    print(f"âœ… å·¥å…·è¿”å›ç»“æœ: {tool_output}")

                    # å°†ç»“æœæ„é€ ä¸º ToolMessage ä¼ å›ç»™ LLM
                    messages.append(ToolMessage(
                        content=tool_output,
                        tool_call_id=tool_call["id"]
                    ))

                # --- æ­¥éª¤ F: ç”Ÿæˆæœ€ç»ˆå›ç­” ---
                final_response = await llm_with_tools.ainvoke(messages)
                print(f"\nğŸ’¡ æœ€ç»ˆå›ç­”: {final_response.content}")

if __name__ == "__main__":
    asyncio.run(run_agent())